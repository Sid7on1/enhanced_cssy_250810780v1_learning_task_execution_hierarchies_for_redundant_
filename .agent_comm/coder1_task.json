{
  "agent_id": "coder1",
  "task_id": "task_6",
  "files": [
    {
      "name": "setup.py",
      "purpose": "Package installation setup",
      "priority": "low"
    }
  ],
  "project_info": {
    "project_name": "enhanced_cs.SY_2508.10780v1_Learning_Task_Execution_Hierarchies_for_Redundant_",
    "project_type": "agent",
    "description": "Enhanced AI project based on cs.SY_2508.10780v1_Learning-Task-Execution-Hierarchies-for-Redundant- with content analysis. Detected project type: agent (confidence score: 11 matches).",
    "key_algorithms": [
      "Integrated",
      "Evolutionary",
      "Used",
      "Kinematics",
      "Each",
      "Task",
      "Maximization",
      "Sot",
      "Dynamically",
      "Optimization"
    ],
    "main_libraries": [
      "torch",
      "numpy",
      "pandas"
    ]
  },
  "paper_content": "PDF: cs.SY_2508.10780v1_Learning-Task-Execution-Hierarchies-for-Redundant-.pdf\nChunk: 1/1\n==================================================\n\n--- Page 1 ---\nJOURNAL 1\nLearning Task Execution Hierarchies for Redundant Robots\nAlessandro Adami\u2217,\u2021, Aris Synodinos\u2020, Matteo Iovino\u2020, Ruggero Carli\u2217, Pietro Falco\u2217\nAbstract \u2014Modern robotic systems \u2014 such as mobile ma-\nnipulators, humanoids, and aerial robots with arms \u2014 often\npossess high redundancy, enabling them to perform multiple tasks\nsimultaneously. Managing this redundancy is key to achieving\nreliable and flexible behavior. A widely used approach is the\nStack of Tasks (SoT), which organizes control objectives by\npriority within a unified framework. However, traditional SoTs\nare manually designed by experts, limiting their adaptability\nand accessibility. This paper introduces a novel framework that\nautomatically learns both the hierarchy and parameters of a\nSoT from user-defined objectives. By combining Reinforcement\nLearning and Genetic Programming, the system discovers task\npriorities and control strategies without manual intervention. A\ncost function based on intuitive metrics \u2014 such as precision,\nsafety, and execution time \u2014 guides the learning process. We\nvalidate our method through simulations and experiments on\nthe mobile-YuMi platform, a dual-arm mobile manipulator with\nhigh redundancy. Results show that the learned SoTs enable\nthe robot to dynamically adapt to changing environments and\ninputs, balancing competing objectives while maintaining robust\ntask execution. This approach provides a general and user-\nfriendly solution for redundancy management in complex robots,\nadvancing human-centered robot programming and reducing the\nneed for expert design.\nNote to Practitioners \u2014In real-world robotics applications such\nas collaborative manufacturing, inspection, or service tasks,\nrobots often have more degrees of freedom than strictly needed\nfor a single task. This redundancy allows them to perform\nmultiple tasks simultaneously, such as holding a tool steady while\navoiding nearby humans or reaching a target while maximizing\nmanipulability. Traditionally, engineers must manually program\ntask priorities and control strategies to exploit this redundancy,\na complex and time-consuming process. This paper introduces a\nlearning-based framework that allows robots to automatically\ngenerate and adapt task hierarchies, using a combination of\nGenetic Programming and RL. The user specifies their objectives\nthrough a simple and customizable cost function (e.g., prioritize\nobstacle avoidance or precision), and the system evolves a \u2018Stack\nof Tasks\u2019 that balances different goals in real time. The method\nis validated on a real ABB mobile YuMi platform, showing how\nthe learned behaviors enable the robot to adaptively coordinate\nmultiple objectives under changing conditions. This framework\nis general and can be applied to any redundant robotic platform,\nreducing integration effort and increasing autonomy in complex\nsettings.\nThese experiments were carried out in the WASP Research Arena (WARA)-\nRobotics, hosted by ABB Corporate Research Center in V \u00a8aster \u02daas, Sweden\nand financially supported by the Wallenberg AI, Autonomous Systems,\nand Software Program (WASP) funded by the Knut and Alice Wallenberg\nFoundation.\nProject co-funded by the European Union \u2013 Next Generation Eu - under the\nNational Recovery and Resilience Plan (NRRP), Mission 4 Component 2,\nInvestment 3.3 \u2013 Decree no. 630 ( 24thApril 2024) of Italian Ministry of\nUniversity and Research; Concession Decree no. 1956 del 05thDecember\n2024 adopted by the Italian Ministry of University and Research, CUP\nD93D24000270003, within the national PhD Programme in Autonomous\nSystems (XL cycle).\n\u2217University of Padova, Dept. of Information Engineering, Italy.\n\u2020Work performed while at ABB Corporate Research, V \u00a8aster \u02daas, Sweden.\n\u2021Polytechnic of Bari Dept. of Electrical and Information Engineering, Italy.Index Terms \u2014Genetic Programming, Reinforcement Learning,\nLearning Stack of Tasks, Redundancy, Task Prioritization, Re-\ndundant Robots\nI. I NTRODUCTION\nMODERN robotic systems are increasingly deployed\nin complex, unstructured, and dynamic environments\nwhere traditional rigid control architectures struggle to meet\nthe operational demands. In particular, mobile manipulators,\nsuch as the mobile-YuMi research platform, exemplify the\nnext generation of robotic systems that combine high dex-\nterity with the ability to navigate and interact with objects\nwithin unpredictable settings. These systems are characterized\nby their high degree of redundancy, which, while affording\nsignificant flexibility and the capability to perform a wide\nvariety of tasks, also imposes substantial challenges in terms\nof control, planning, and task prioritization. Redundancy in\nrobotic manipulators implies that the system possesses more\ndegrees of freedom (DOF) than strictly necessary to execute a\nspecific task. This surplus of DOF can be exploited to optimize\nmultiple objectives, such as avoiding obstacles, maximizing\nmanipulability [26], or maintaining safe distances from me-\nchanical joint limits. However, managing this redundancy is\nnon-trivial, particularly when the robot is required to perform\nmultiple tasks concurrently. Deciding which of many possible\ntask execution strategies to follow is a highly challenging prob-\nlem and an active research field. In [9], the authors present a\nunified control framework for redundant robots that addresses\nkinematic and dynamic control while handling hierarchical\nconstraints and optimizing redundancy through a generalized\nnull space algorithm. In [36], the authors present an online\nkinematic control approach for mobile bimanual manipulation\nin dynamic environments, using distributed distance sensors\nand priority-constrained optimization to enable real-time col-\nlision avoidance and task execution. In current practice, the\nredundancy management problem is handled manually by ex-\npert programmers who hard-code task sequences and decision\nrules. This manual approach is not only time-consuming and\ncostly, but also inflexible: adapting the robot to a new mission\nor environment typically requires reprogramming the entire\ntask logic. To address these limitations, we propose a method\nthat automatically learns the stack of tasks required to fulfill\na high-level mission by using a cost function to guide task\nparameters and prioritization tuning. This approach reduces\nreliance on manual tuning and enables scalable, adaptive robot\nbehavior across different scenarios.\nConcurrently, Genetic Programming has emerged as a power-\nful evolutionary method for automatically generating control\nprograms [12]. Unlike traditional genetic algorithms that oper-\nate on fixed-length strings [15], Genetic Programming evolves\nvariable-length programs, thus capturing the hierarchical andarXiv:2508.10780v1  [cs.RO]  14 Aug 2025\n\n--- Page 2 ---\nJOURNAL 2\nReinforcement Learning\nRL Simulation\nSoTs are tested\non the same task\nFitness evaluation\nCost is assigned to each SoTGenetic Programming\nFitness minimizer\nGenetic Selection\nReduce population\nGenetic Recombination\nRestore population sizeLaboratory Tests\nTest of the best\nStack of Tasks\nFramework Validation\nwith laboratory tests\nAfter fitness eval. Genetic Programming is appliedBest SoT\nis selectedNew SoTs\nFigure 1. Overall schematic representation of our framework. RL serves as the basis for Genetic Programming to develop a solution for the given problem.\nFirst, all the Stack of Tasks are executed, and a cost function is assigned through RL. Then, Genetic Programming evolves new potential solutions, which\nreturn a lower cost function. The process is iterated until a stationary point is reached, and then the best Stack of Tasks is found. Finally, the evolved SoT\nstrategies were tested on a real mobile dual-arm robot (mobile YuMi produced by ABB).\ndynamic nature of complex tasks [34].\nThis paper proposes an integrated framework that leverages\nboth RL and Genetic Programming to address the dual chal-\nlenges of task prioritization and redundancy management in\nmobile manipulators (as shown in Fig. 1). Central to our\napproach is the introduction of the \u201cStack of Tasks\u201d (SoT)\nrepresentation in Genetic Programming, a structured format\nthat encapsulates the priority order of each task that comprises\nthe high-level mission, associated control parameters, and the\nactivation status within a single composite entity. The SoT not\nonly provides a natural and efficient means of representing\ntask hierarchies, but also facilitates the application of genetic\noperators, such as mutation and crossover, to evolve and\noptimize the task stack. The evolution of the SoT is guided by\na user-defined cost function, which is expressed as a weighted\ncombination of multiple performance metrics. These metrics\nare defined during the algorithm development in order to\nexploit features of the robot that may be useful in a real-\nworld scenario. They include precision (measured by the\ndeviation from a desired pose), safety (evaluated based on\nthe proximity to obstacles), manipulability (which is strictly\nconnected to kinematic performances), and execution time.\nThe integration of RL with Genetic Programming within our\nframework offers several distinct advantages. First, it allows\nthe robotic system to handle the redundancy automatically,\nwithout designing the stack manually, enhancing robustness\nand flexibility. Second, the evolutionary aspect of Genetic\nProgramming enables the automatic discovery of optimal\ntask sequences and control parameters, reducing the need for\nmanual tuning and prior knowledge. Third, by explicitly in-\ncorporating a user-defined cost function, our approach ensures\nthat the optimization process remains aligned with the specificobjectives and constraints of the application at hand. Because\nthe task order and parameters are learned independently of\nthe environment, they can be generalized to any real-world\nsetting where the same mission and cost function apply. This\nenables scalable, adaptive robot behavior without the need for\nmanual reprogramming. The proposed framework is validated\nthrough a comprehensive experimental study that includes\nboth simulation and real-world experiments. Simulations are\nconducted using the Gazebo environment, where the robot\noperates within a strictly defined dynamic environment. These\nsimulations are crucial in evaluating the convergence prop-\nerties of the Genetic Programming algorithm and in fine-\ntuning the RL parameters before deployment on physical\nhardware. Subsequent experiments on the mobile-YuMi re-\nsearch platform further demonstrate the practical viability of\nthe approach, showcasing its ability to adapt to unforeseen\nchanges, such as moving obstacles and variations in task\npriorities, while ensuring stable and efficient operation.\nIn summary, the main contributions of this paper are\ntwofold.\n\u2022The core contribution of this work is a method for\nlearning the structure of a SoT \u2014 including task ordering,\nparameter tuning, and task activation \u2014 for redundant\nrobots, based on high-level user-defined cost functions.\nIn particular, we frame the problem as policy search in\na reinforcement learning-style setting, where the policy\ncorresponds to a structured SoT. Genetic Programming\nis used as the global optimizer to discover effective SoT\nconfigurations. Unlike conventional RL, we do not train a\npolicy function: instead, we search directly in a discrete-\ncontinuous policy space using episodic cost as feedback.\n\u2022Because the task order is automatically learned with task\n\n--- Page 3 ---\nJOURNAL 3\nparameters \u2014 rather than manually designed or tailored\nto a specific environment \u2014 it can be directly generalized\nto different real-world settings where the same high-level\nmission and cost function apply. This fully automated\napproach eliminates the need for manual reprogramming,\nenabling scalable and adaptive robot behavior across\ndiverse deployment scenarios. We demonstrate the prac-\ntical efficacy of our approach through simulations and\nlaboratory experiments, highlighting its robustness in dy-\nnamic and unpredictable environments while maintaining\na strong emphasis on safety and performance.\nThe remainder of this paper is organized as follows. Section II\nreviews related work in redundancy management and task\nprioritization in robotic systems. Section III and IV detail the\ntheoretical framework underpinning our integrated approach,\nincluding a discussion of the RL and Genetic Programming\nmethodologies. Section V describes the experimental setup,\nincluding the simulation environment and the mobile-YuMi\ntestbed. Section VI presents and analyzes the experimental\nresults, and Section VII concludes the paper while outlining\ndirections for future research.\nII. R ELATED WORKS\nThe SoT framework has been widely used in robot control\nfor managing redundancy and task prioritization. A significant\nbody of literature has demonstrated the versatility and effec-\ntiveness of SoT in various contexts. However, the definition of\nthe tasks and their hierarchical structure is typically performed\nmanually. Authors in [24] introduced a generalized SoT frame-\nwork for humanoid robot control, where tasks such as position,\norientation, and posture were manually specified and prior-\nitized. Similarly, in the context of dynamic motion capture,\nin [31] the SoT paradigm is applied to reproduce and edit\nhuman motions on robots. In later work [23], SoT formulation\nis extended to incorporate unilateral constraints such as contact\nforces. While this allowed for more expressive control, the\ndefinition and prioritization of tasks and constraints remained\nmanual. Authors in [16] integrated visuo-tactile perception into\nthe SoT framework to enable fine manipulation. The resulting\ntasks were still manually defined based on sensor interpretation\nand task heuristics. More recently, in [5] it was proposed to\ncombine SoT with Behavior Trees to introduce modularity\nand high-level decision-making. Nevertheless, even in this\nhybrid architecture, individual tasks within the SoT were\nexplicitly specified by the developer. While the Stack of Tasks\n(SoT) paradigm is widely used for redundancy management\nin robotics, task priorities, control parameters, and activation\nstructures are still typically manually designed by experts,\nlimiting scalability and adaptability in dynamic environments.\nSome recent works introduce data-driven methods \u2014 such as\nReinforcement Learning or Behavior Trees \u2014 to automate\ntask selection or control policy generation, but they stop\nshort of explicitly learning full SoT structures. In recent\nyears, Genetic Algorithms (GA) and Genetic Programming\ncombined with Behavior Trees (BT) have been effectively\nused in robotics to address the challenges associated with\nredundancy for the control and path planning of robots withmore degrees of freedom than needed. This characteristic\nallows great flexibility but also introduces complexity in task\ncomputation. In [27], the authors address the challenge of\npoint-to-point motion in redundant manipulators operating\nin environments with obstacles. GAs are employed to find\noptimal paths that avoid collisions while ensuring precise\nmovement. In [28], the authors present a modified GA to\naddress the inverse kinematics problem in redundant robot\nmanipulators working in environments with obstacles. The\napproach formulates the problem as an optimization task, aim-\ning to minimize both the positional error of the end-effector\nand the joint displacements of the robot, showing improved\naccuracy over traditional methods. Furthermore, hierarchical\ntask prioritization has been explored extensively, with methods\nsuch as null space projection and behavior trees providing\na framework for managing redundant degrees of freedom\nin manipulators [14, 6]. These methods have shown that\nsuch hierarchical structures not only improve task execution\nreliability but also facilitate smoother integration of multiple\nconcurrent tasks. In [4], the authors propose a two-layer\nmotion planning framework that combines a sampling-based\nCartesian-space planner with set-based inverse kinematics to\nenable efficient and reactive control of redundant manipulators\nin dynamic, partially unstructured environments. However, its\nperformance may be limited by reliance on accurate perception\nand environmental modeling, and scalability challenges may\narise in complex, high-dimensional tasks that affect real-time\nresponsiveness.\nEmerging techniques, including deep Reinforcement Learning\n(deep RL), have demonstrated significant promise in handling\ncomplex decision-making problems in robotics [19]. Deep\nRL can learn robust control policies directly from high-\ndimensional sensor data. However, these methods often lack\nexplicit representations for task prioritization, making it diffi-\ncult to interpret and adjust the control hierarchy when safety\nor precision requirements change. However, these works do\nnot aim to automatically discover or optimize SoT hierarchies\nwith task-specific null-space projections and control-level pri-\noritization. In contrast to these existing approaches, our work\nintroduces an integrated framework that automatically handles\nredundancy management using a sim-to-real transfer without\na fine-tuning method based on Genetic Programming.\nIn most state-of-the-art robotics systems, redundancy man-\nagement is handled by an expert programmer, who manually\ndefines the SoT. However, this manual approach is expensive\nand inflexible. On top of that, manually designing SoTs for\nhighly-redundant robots such as dual-arm mobile manipulators\nis impractical because it becomes overly complex and unman-\nageable as the number of tasks increases [5]. Our approach\nnot only evolves the optimal order of task execution, but\nalso adapts control parameters in real time to minimize a\nuser-defined cost function. By dynamically learning both task\npriorities and associated parameters, the proposed framework\ndirectly addresses the limitations in prior methods of static task\nhierarchies and manual tuning. To the best of our knowledge,\nno prior work jointly learns the task order, control parameters,\nand task activation flags of a Stack of Tasks architecture for\nredundant robots using a performance-driven approach. Our\n\n--- Page 4 ---\nJOURNAL 4\nwork fills this gap by treating SoT as a structured policy\nrepresentation and evolving it based on user-defined cost\nfunctions, enabling sim-to-real transfer without fine-tuning and\nrobust behavior adaptation in complex scenarios automatically.\nIII. P ROBLEM STATEMENT AND PROPOSED SOLUTION\nManaging the redundancy of robotic manipulators with\nmultiple degrees of freedom (DOF) is a critical challenge.\nRedundancy in a robotic manipulator occurs when there are\nmore degrees of freedom than are strictly necessary to perform\na given task. Formally, for a manipulator with ndegrees of\nfreedom operating in a task space of dimension m, redundancy\nis defined as the condition where n > m . As an example,\nconsider a 7-DOF robotic arm (namely, an arm with seven\nrevolute joints) tasked with positioning its end-effector in a\nspecific pose in 3D space defined as an element of SE(3).\nSince specifying a unique pose in SE(3)requires m= 6\nparameters (three for position and three for orientation), only\n6 DOFs are necessary to fully constrain the task. Therefore, a\n7-DOF arm ( n= 7) is said to be kinematically redundant,\nas it possesses one extra degree of freedom beyond what\nis required for the task. Such redundant manipulators admit\ninfinite joint configurations that achieve the same end-effector\npose, making them highly versatile for constrained or dynamic\nenvironments [33]. To choose one among the infinite joint\nconfigurations that solve the task mentioned above, we can\nintroduce a secondary task to fulfill at the same time, for\nexample, maximizing the manipulability. This way, we define\na simple stack of two tasks. It is important to note that the\nSoT approach allows for handling at the same time multiple\ntasks exploiting the redundancy. Thus, redundancy can be\nexploited to optimize multiple objectives, such as avoiding\nobstacles, minimizing energy consumption, or avoiding getting\nclose to joint limits. However, while redundancy management\nis a well-studied problem, our focus lies in automatically\nhandling this additional freedom to determine which of the\nmany possible solutions best aligns with the overall system\ngoals. This requires not just advanced control strategies but a\nmechanism for autonomously selecting and prioritizing actions\nbased on task parameters and mission objectives. Although\nthis redundancy offers the potential for greater flexibility and\nadaptability, it also introduces complexity in decision-making\nbecause the robot can achieve the task with infinitely many\nconfigurations with varying degrees of efficiency, precision,\nencumbrance, and other performance indices. In dynamic and\nunpredictable environments, the ability of a robot to adapt\nits movements is crucial to achieving optimal performance.\nWithout a robust method to select the most appropriate config-\nuration, the benefits of redundancy can be easily lost, leading\nto inefficient results or even failure in task execution.\nThe central problem we address is the automatic derivation\nof an optimal task execution management referred to as a\nSoT \u2014 that allows a robot to successfully complete a high-\nlevel mission in dynamic or unknown environments. Rather\nthan relying on manually defined task orders, which are often\ninflexible and environment-specific, our objective is to learn\nthis stack from simulation and transfer it directly to realenvironments, using task parameters and a cost function to\nguide the prioritization. This enables adaptive, scalable, and\ncontext-aware redundancy management without the need for\ntraditional hard-coded SoT formulation.\nThe proposed solution (schematized in Fig. 1) comprises\nRL and Genetic Programming techniques, combining and\nenhancing the characteristics of both methodologies to find\nan optimal solution for the problem.\nThis framework optimizes the execution and prioritization of\ndifferent tasks to perform a global mission while minimizing\nthe cost associated with individual tasks. The SoT architecture\nrepresents the tasks that the robot is allowed to execute by\nencoding the priority order of their execution, while carrying\ninformation about the parameters and the fitness measure.\nIn our solution, RL and Genetic Programming are exploited\nto optimize the SoT structure representation of a high-level\nmission. The RL component enables the system to learn\noptimal SoTs through environmental feedback using the Ge-\nnetic Programming optimizer. Specifically, the agent receives\na reward based on the performance metrics defined by a\nuser-specific cost function. This reward signal is then used\nto update the SoT according to established algorithms [37].\nThe cost function to be minimized is customized by the user\nbased on the specific priorities of the application, such as\nminimizing time or maximizing precision. Genetic Program-\nming is currently employed to evolve the structure of the SoT.\nGenetic operators such as mutation and crossover are applied\nto candidate solutions, each encoding a particular task priority\nand associated control parameters. The fitness evaluation in\nthe Genetic Programming process is directly informed by the\nRL-derived cost function.\nInitially, all SoTs are executed, and RL assigns a corre-\nsponding cost function. Genetic Programming then explores\nnew potential solutions aimed at minimizing this cost. This\niterative process continues until a stationary point is reached,\nat which stage the optimal Stack of Tasks is determined. This\nintegration ensures that the evolutionary process favors SoTs\nthat not only achieve low cost but are also adaptive to the\ndynamic feedback provided by the RL component.\nIn the algorithmic pipeline, the user defines a cost function C\nbased on the requirements that must be satisfied in a specific\nuse case. For ease of use, the user directly specifies the\ndesired high-level requirements and their weights for the given\nscenario. Then, the choices are automatically mapped into a\nconvex cost function. Moreover, state-of-the-art approaches\ncan be adapted and used to generate reward functions directly\nfrom natural language task descriptions [38]. Then, an initial\npopulation of SoTs candidates of size nT\u2014 where nTis\nthe number of tasks \u2014 is randomly created, and through RL,\nthe cost is evaluated for each of them. The same high-level\nmission is executed with randomized initial conditions for all\nof the candidates until the goal is reached or the execution\nfails.\nOnce all SoTs of the current population are evaluated, Genetic\nProgramming techniques are used to create new individuals\nand potentially find a better solution to the problem. As\na first step, genetic selection is applied, pairing the SoTs\nand propagating only the one with the lowest cost (namely,\n\n--- Page 5 ---\nJOURNAL 5\nthe one with the best performance) in the next generation\nand discarding the least effective solution, as it does not\ncontribute to improving the overall performance. Therefore,\nthe framework employs tournament selection [8], a common\nmethod in Genetic Programming, to evolve better task prior-\nitization strategies. This binary tournament approach ensures\nthat better-performing solutions are more likely to propagate,\nguiding the search towards more efficient and adaptive task\nmanagement policies.\nIn the second step, the initial amount of individuals will be\nrestored through Genetic Programming operations (crossover\nFig. 3 and mutation Fig. 4). Then, new SoTs individuals will\nbe evaluated as well.\nThis loop continues until the fitness scores converge to a\nstationary value, meaning that the evolved SoT individuals are\nalmost equal to each other. Two different SoTs are considered\nto be almost equal when their distance d(see Sec. IV-E) is\nbelow a threshold that is chosen manually based on empirical\nobservation.\nIn this context, the use of Genetic Programming provides\na framework for the robot to learn task prioritization and\nparameters. The robot explores various potential solutions,\ncontinuously refining its approach to achieve the optimal\nbalance of task execution.\nAfter that, the best SoT \u2014 the one with the lowest cost in\nthe final population \u2014 will be tested in a real laboratory\nenvironment, to prove the effectiveness of the algorithm in\nfinding an optimal solution for a high-level mission.\nThe pseudo-code of the proposed algorithm is reported in\nAlg. 1, while more details on the theoretical framework and\nthe techniques used are reported in the following section.\nAlgorithm 1 Pseudo-code of the proposed solution\nThe user defines a cost function C\nA random initial population of SoT pop is created\nwhile a stop condition is not reached do\nExecute all the tasks without assigned cost Cinpop\nCouple the SoTs and apply Genetic Selection\nforeach survived SoT do\nnew pop\u2190SoT\nSelect a genetic operation genopfrom subset\n{crossover, mutation }\nifgenopiscrossover then\nSelect a second survived SoT\nMerge the two SoTs in a new individual SoT\u2032\nend if\nifgenopismutation then\nMake a change in SoT and get SoT\u2032\nend if\nnew pop\u2190SoT\u2032\nend for\nClear pop\npop\u2190new pop\nend while\nThe best SoT in popis selected as solution of the problemIV. T HEORETICAL FRAMEWORK DETAILS\nIn this section, we provide details about the theoretical\nframework illustrated in the above section, with a focus on\nthe methodology and the implementation.\nA. Tasks\nThe robot can execute tasks selected from a predefined\ndictionary D={T1, T2, ... , T N}. For each task T, we define\nthe task variable to be controlled as x\u2208Rmand the system\nconfiguration as q\u2208Rn. Let us represent the relationship\nbetween xandqwith the function k:\nx=k(q). (1)\nUnder the assumption that kis differentiable, we have:\n\u02d9x= J(q)\u02d9q=\u2202k(q)\n\u2202q\u02d9q, (2)\nwhere \u02d9qand\u02d9xare the time derivatives of qandxrespectively,\nwhile J(q) is the Jacobian associated with the task T. Given\na desired trajectory xd(t)for the task variable, fulfilling a\ntask ideally means to generate a trajectory qd(t)such that\nk(qd(t)) = xd(t). An effective way to generate motion\nreferences qd(t)for the robot system starting from desired\nvalues xd(t)\u2014 and possibly \u02d9xd(t)\u2014 is to act at differential\nlevel by computing velocity references \u02d9q(t)for the robot\nsystem [1]. Depending on the particular type of task, we\ncompute \u02d9q(t)using one of the two following approaches.\n\u2022Closed loop : the reference generation for this group\nof tasks is based on the CLIK (Closed-Loop Inverse\nKinematics) approach [1] [7] and foresees closed-loop\nintegration of the error associated with the task with a\nfeedback mechanism:\n\u02d9qd(t) = J\u2020(q(t)) ( \u02d9xd(t) +\u03b3CL(xd(t)\u2212k(q(t)))) (3)\nwhere J\u2020= (JTJ)\u22121JTis the Moore\u2013Penrose pseudo-\ninverse and \u03b3CLis the feedback gain. At each time\ninstant t, the feedback error is calculated as the difference\nbetween the desired task quantity xd(t)and the current\nonek(q(t)). The introduction of the derivative \u02d9xdand the\ndependence on time of the desired task trajectory allow\nus to define a trajectory that leads the system from an\ninitial pose x0to the goal pose xdin the time interval\n[0, ts](trajectory time). An example of a closed-loop task\nis Cartesian tracking via inverse kinematics.\n\u2022Open loop : these tasks do not implement the feedback\nmechanism. The optimization objective is to minimize\nor maximize the task variable, which is usually a scalar.\nTo distinguish from closed-loop tasks, we indicate the\nrelationship between task and configuration variable with\nthe function w. We use k(q)in closed-loop tasks to\ndescribe task variables that follow a desired trajectory or a\ngoal, whereas w(q)is used in tasks that optimize a scalar\nobjective, such as manipulability. Therefore, following\nthe gradient-based approach in [33], we express the\n\n--- Page 6 ---\nJOURNAL 6\ndesired joint velocities as a partial derivative of w(q(t))\nscaled by the gain \u03b3OL:\n\u02d9qd(t) =\u03b3CL\u0012\u2202w(q(t))\n\u2202q(t)\u0013T\n. (4)\nAn example of an open-loop task is maximizing manip-\nulability or distance from joint limits.\nMore generally, we can characterize each task Twith a\nparameter \u03b8, which is included in the learning objective.\nAccording to this parametrization, in closed-loop tasks we\nhave \u03b8=\u03b3CL, while in open-loop tasks we have \u03b8=\u03b3OL.\nB. Null-Space Projection\nTasks can be stacked in an ordered tuple, which is im-\nmutable once the stack is deployed on the robot. From this,\na task with a lower priority can iteratively be projected into\nthe null space of another with higher priority [26] [33] using\nJacobian matrices. By taking two tasks as an example, the\nprojection is defined as:\n\u02d9qd=\u02d9q1+\u0010\nIn\u2212J\u2020\n1J1\u0011\n\u02d9q2. (5)\nwhere \u02d9q1and \u02d9q2are the joint velocity commands obtained\nfrom solving the primary and secondary tasks, respectively,\nandJ1is the Jacobian matrix associated with the higher\npriority task.\nWhen a robot has to execute multiple tasks simultaneously,\nnull space projection is a crucial tool to ensure that the\nsecondary tasks do not interfere with the primary ones.\nC. Cost function\nThe cost function Cis a crucial aspect of this framework,\nas it defines the performance of different SoTs in the same\nscenario. This allows for embedding user preferences and\nenhancing the ease of use. Moreover, it is a key component\nof Genetic Programming processes in RL frameworks, as\nit serves as a basis for genetic selection and performance\ncomparison. The cost function can be defined by the user\nas a weighted combination of predefined cost operators that\nregulate the robot\u2019s priorities while performing a certain com-\nbination of tasks. The user can combine the costs with the\nrelative weights as:\nC=\u03b11cost 1+\u03b12cost 2+...+\u03b1ncostn, (6)\nwhere weights are normalized such thatPn\ni=0\u03b1i= 1. This\nfunction is pivotal in determining the parameters to optimize\nduring the learning phase, while taking into account user\npreferences.\nThis implementation allows the cost function to be tailored to\nthe specific requirements of the application, ensuring that the\noptimization process aligns with both operational objectives\nand safety constraints.\nThe user is allowed to select the high-level requirements and\npreferences that define the cost function at the beginning of the\ntraining phase through a Graphical User Interface, as described\nin Sec. V-C. Therefore, once the user defines the weights for\nthe fitness measure, C, the framework automatically evolves\na SoF in a simulated environment.D. Stack of Tasks\nThis work aims to automatically manage the redundancy\nof complex robotic systems by learning an optimized and\nprioritized order for a set of tasks using Genetic Programming\nand RL techniques, guided by a user-defined cost function that\nreflects the specific performance criteria relevant to the task,\nsuch as minimizing execution time or prioritizing perceived\nsafety.\nIn the following, the Genetic Programming framework is\napplied to a SoT structure. Each individual in the population of\npotential best solutions of our problem is represented as a stack\nin which the task descriptions are stored together with their\nparameters. The specific order in the data structure defines\nthe task priorities, which are guaranteed through null space\nprojection.\nIn the implementation, the first position in the stack, namely\nat index 0in the array of Fig. 2, the value associated with the\ncost function is reported. So, all stacks carry all the necessary\ninformation to compare them (a cost function C) and execute\na high-level mission (a set of parameters \u03b8and a priority order\nP).\n[C,[1sttask] ,[2ndtask] ,[3rdtask] , ...]\nFigure 2. Stack of Task (SoT) representation. The cost associated with the\nSoT execution is placed at the beginning of the representation, followed by\nthe tasks ordered according to their priority.\nIn Fig. 2, a simple example of SoT is reported. In this\ncase, the task labeled as 1stis fully executed with the highest\npriority, the task labeled as 2ndis then projected in the null\nspace of the first, while the 3rdis projected in the combined\nnull space of the two tasks with higher priority [26]. Note\nthat other tasks may be concatenated in the SoT structure.\nPriority order, active tasks, and parameters can be changed\nand combined with Genetic Programming to find the solution\nthat minimizes the cost function.\nThe parameters, denoted as \u03b8, are restricted to be in a specific\nrange dictated by prior knowledge of the task. For example,\nthe Inverse Kinematic gain is constrained to be positive and\nlower than 2to fulfill necessary stability conditions [7] for\nsafety reasons.\nWe introduce a biologically inspired Boolean activation mech-\nanism for task inclusion in a prioritized control framework.\nThis mechanism, based on introns [40], allows for the preser-\nvation of latent task modules\u2014analogous to recessive alleles\nin genetics\u2014enabling exploration without structural disrup-\ntion [29]. To our knowledge, this approach has not been\nformally applied to motion control or Stack of Tasks frame-\nworks, and provides a novel method for retaining potentially\nbeneficial behaviors during evolutionary optimization. Each\ntask in the control hierarchy is associated with a Boolean\nactivation flag that determines whether it contributes to the\noverall control output. When the flag is set to True , the task\nis considered active and participates in the computation of\nthe desired joint velocities via its corresponding projection\nin the Stack of Tasks. If the flag is False , the task is\nskipped in the projection computation and does not influence\n\n--- Page 7 ---\nJOURNAL 7\nthe resulting motion, but its parameters are preserved in the\ngenotype. These inactive tasks function as latent traits; they are\nnot expressed in the current behavior but may become active\nin future generations through mutation or recombination. This\nmechanism enables the evolutionary algorithm to vary not\nonly the parameters of the tasks but also the effective length\nand composition of the control stack, without permanently\ndiscarding potentially useful behaviors. It supports a more\nflexible search process by allowing solutions to retain and later\nreintroduce tasks that may become advantageous in different\nenvironmental contexts or task configurations. As a result, the\npopulation maintains greater behavioral diversity over time,\nwhich can reduce the risk of premature convergence and\nimprove the robustness and generalization of the final solution.\nThe cost is evaluated once the goal of the general high-\nlevel mission is reached, or a maximum amount of time has\nexpired, or a breaking point of the simulation is met (the robot\ncollides with an obstacle, or an arm configuration reaches a\nsingularity). Once the cost is computed, it can be used for\ngenetic selection.\nThen, a genetic operation is applied to the SoT structure.\nIn case of a mutation, a parameter, the priority order of the\ntasks, or the presence of a task in the stack can be randomly\nchanged, producing a new offspring which is added to the next\npopulation. If crossover is applied instead, another survived\nSoT is randomly chosen as the second parent, and the offspring\nis then derived. Therefore, the proposed framework combines\nGenetic Programming and RL to find the set of priority order\nPand parameters \u03b8that minimizes the cost function C:\n\u02c6P,\u02c6\u03b8= argmin\nP,\u03b8C. (7)\nEach Stack of Tasks can be formally represented as a map S,\nwhich maps the priority order, parameters, and tasks Tfrom\nthe dictionary Dto the desired joint velocity, given the cost\nfunction input by the user:\nS: (P,\u03b8, T|C)\u2212\u2192 \u02d9qd. (8)\nIn summary, SoT is a structured representation in which each\nelement of the stack corresponds to an individual task. Each\ntask entry comprises:\n\u2022Priority Level: Determines the execution order, with\nhigher-priority tasks executed first.\n\u2022Control Parameters: Specific settings for task execution.\n\u2022Activation Flag: Indicates whether a task is active or\ninactive, allowing dynamic modification of the stack\nwithout losing potential task information.\nE. Distance between Stacks of Tasks\nGiven two different SoTs aandb(of length n), the distance\nbetween them \u2014 which serves to quantify how dissimilar the\ntwo solutions are \u2014 can be computed as:\nd(SoTa, SoT b) =ntasksX\ni=1\u0000\n\u03b4\u03b8(\u03b8i\na, \u03b8i\nb) +\u03b4prior(Ti\na, Ti\nb)\u0001\n,(9)\nfor all the ntasks Tipresent in the list. \u03b4\u03b8(\u03b8i\na, \u03b8i\nb) =\u2225\u03b8i\na\u2212\u03b8i\nb\u2225\nreturns the sum of the differences between the parameters oftwo different realizations of the same task in different SoTs\n(namely Ti\naandTi\nb) and\n\u03b4prior(Ti\na, Ti\nb) =(\n|\u03c0i\na\u2212\u03c0i\nb|ifTi\naandTi\nbare both active\nnotherwise\n(10)\nwhere \u03c0i\naand\u03c0i\nbare the priority positions of task Ti\naandTi\nb\nin the set of active tasks for each SoT.\nF . Genetic Programming as Reinforcement Learning cost min-\nimizer\nGenetic Programming [12] is a computational methodology\ninspired by the principles of natural evolution. Its primary\ngoal is to automatically generate computer programs that can\neffectively solve specific tasks. Unlike Genetic Algorithms\n(GA) [15], which operate on genomes encoded as fixed-\nlength strings, Genetic Programming evolves variable-length\nprograms, allowing for more flexible and dynamic solutions.\nFor our purposes, Genetic Programming serves as a basis\nfor the development of RL algorithms [32, 37], allowing us\nto minimize the fitness measure. Unlike supervised learning,\nwhere models rely on labeled datasets, RL agents refine their\nstrategy by receiving feedback in the form of rewards in\nan iterative trial-and-error process. Thus, fitness evaluation\nis a crucial component of RL and has undergone significant\nimprovements with the integration of Genetic Programming\nparadigms [25, 2, 11, 39, 21, 42]. The integration of Genetic\nProgramming with RL techniques has opened new avenues\nfor developing adaptive policies and strategies in dynamic\nenvironments [37]. Genetic Programming can be employed to\ndirectly evolve policies, representing them as programs. Our\nframework can be interpreted as an RL approach, where a\npolicy (SoT) is selected, executed in an environment (simula-\ntion or real robot), and evaluated via a scalar cost. However,\nrather than learning with gradients or value functions, we use\nGenetic Programming as the policy optimizer. This enables\npolicy search over interpretable task hierarchies. In our work,\nwe use the method as a zero-shot sim-to-real learning approach\nsince the robot learns in simulation and does not need to\nperform fine-tuning on the real environment.\nTraditionally, Genetic Programming represents programs as\ntree-based structures [41, 35] (as in Fig. 3 and Fig. 4), reflect-\ning the hierarchical nature of many biological languages [14,\n13]. Furthermore, the ability of Genetic Programming to\nevolve temporally extended actions enhances an agent\u2019s ca-\npacity to learn long-term strategies, a critical factor in many\nreal-world applications. In practical applications, Genetic Pro-\ngramming has demonstrated its versatility and effectiveness in\na wide range of domains [18, 30, 17]. In robotics, for example,\nGenetic Programming is utilized to evolve control programs\nthat enable robots to exhibit adaptive and resilient behaviors\nin dynamic environments [13, 20].\nA fundamental mechanism in Genetic Programming is the ap-\nplication of genetic operators [12], particularly crossover and\nmutation after genetic selection, which drives the evolutionary\nprocess by generating variation in the population. Genetic\nselection allows for selecting the best algorithms to thrive in\n\n--- Page 8 ---\nJOURNAL 8\nthe process, while the worst are discarded. This process will be\nbased on the fitness measure associated with the individuals.\nCrossover is a recombination operator that exchanges genetic\nmaterial between two parent programs to create offspring.\nIn tree-based Genetic Programming, crossover is typically\nimplemented by selecting a random sub-tree in each parent\nand swapping these sub-trees to produce new individuals. This\nmechanism enables the exchange of functional building blocks\nbetween solutions, facilitating the discovery of novel program\nstructures. Mathematically, given two parent programs P1and\nP2, the crossover operation (schematized in Fig. 3) can be\nexpressed as:\nO=crossover (P1, P2) (11)\nwhere the resulting offspring Oinherits traits from both\nparents, preserving useful characteristics while potentially\nintroducing diversity.\nP1 P2\nO\nFigure 3. Example of crossover with a tree structure. From two parents P1\nandP2, an offspring Ois generated by shuffling their characteristics.\nMutation, on the other hand, is an operator that introduces\nsmall, random changes to a program to enhance diversity and\nexplore new regions of the search space, while preventing\npremature convergence to a local optimum. In tree-based\nGenetic Programming, mutation typically involves selecting\na random node and replacing it with a newly generated node\nor sub-tree. Formally, the mutation (schematized in Fig. 4) can\nbe defined as:\nO=mutation (P1) (12)\nwhere Ois a variant of P1with an altered structure that\nencourages adaptation and refinement over successive genera-\ntions.\nP1 O\nFigure 4. Example of mutation with a tree structure. From a parent P1, an\noffspring Ois generated, changing its characteristics.\nV. E XPERIMENTAL SETUP\nIn this section, we introduce the setup of the learning phase\n\u2014 where a simulated environment is used \u2013 and the testphase \u2014 which is performed with a real robot in a controlled\nlaboratory environment.\nThe robot model chosen for the test phase is the mobile-YuMi\nresearch platform, available at ABB Corporate Research in\nV\u00a8aster \u02daas, Sweden. Its architecture is based on the Dual-Arm\nABB YuMi, which is a bimanual manipulator with 7 DOF\nfor each arm. The manipulator is installed on top of a mobile\nbase, adding navigation capabilities. The base itself is treated\nas 3 additional DOF elements (translation along xandyaxes\nand rotation around the zaxis), which can be added to the\nkinematic chain or used as a single entity to move the platform.\nOnly the mobile base is equipped with distance sensors (a pair\nof LiDAR scanners, one at the front and one at the rear of the\nmobile base). Thus, it was possible to deal with the obstacle\navoidance task only in the case where the base was involved,\nas the arm is bereaved of distance sensors.\nThe learning phase was conducted through repeated simula-\ntions in a Gazebo environment, while the real-world tests were\ncarried out at ABB Corporate Research. Both the simulated\nand the real robots can be controlled by applying joint ve-\nlocities to the arms and 3 velocities to the base \u2014 linear\nvelocities along xandyand angular velocity around z\u2014\nwhich are translated by the robot software into steering and\nspinning velocities for the wheels. The interfaces to the real\nand the simulated robots were implemented in ROS2 Humble.\nA. Proposed Tasks\nIn our experimental evaluation, the set of tasks that the robot\nis allowed to perform is explicitly defined in a dedicated task\ndictionary. This dictionary serves as a structured reference that\nenumerates and categorizes all permissible tasks. For clarity\nand analytical purposes, we divide these tasks into two main\ngroups, each corresponding to a distinct type of interaction or\nobjective within the evaluation framework.\nOne includes useful tasks for the execution of a high-level\nmission, and the other includes tasks that do not contribute\nto the completion of the high-level goal and are therefore\ndisturbing the execution. Non-relevant tasks are introduced\nin the framework to prove the robustness of the proposed\nsolution, given a cost function. Both groups are made up of 4\ndifferent tasks, and all of them return a desired velocity vector\nfor the joints \u02d9qd\u2208Rn.\nThe set of useful tasks is composed of the following:\n\u2022Inverse Kinematic (IK) with a trajectory time tas\nrepresented in Fig. 5. In this work, the desired velocity for\nthe inverse kinematics (IK) task is not determined solely\nby the pose error eIK=kd\u2212k(q(t))\u2208R6between the\ndesired pose and the current one. Instead, a trajectory is\ngenerated by selecting a sequence of intermediate points\nfrom the initial pose to the target pose over a predefined\nduration (set as a task parameter). The desired velocity\nat each time step is then computed to drive the system\ntoward the next point along this trajectory. Thus, at each\ntime step, the end-effector (or the mobile base) attempts\nto reach a specific point of this trajectory.\nThe input z(t)of the task is given by the desired end-\neffector pose kd, which is time variant as it is computed\n\n--- Page 9 ---\nJOURNAL 9\nthrough a trajectory, and the desired Cartesian space\nvelocities. The desired velocity of the task is given by\nthe solution of the following equation:\n\u02d9qd(t) = J\u2020(q(t))\u0010\n\u02d9kd(t) +\u03b3CL(kd(t)\u2212k(q(t)))\u0011\n,\n(13)\nwhere k(t)\u2208R6is the vector describing the pose of the\nend-effector at time t,kd\u2208R6is the desired target pose\nand\u02d9kdits derivative, which are computed following the\ndesired trajectory of Fig. 6. J\u2208R6\u00d7nis the Jacobian\nmatrix of the kinematic chain involved in the task.\nFigure 5. Inverse Kinematic sequence of the mobile-YuMi research platform.\n0 5 10 15 20 25 30\nT ime [s]\u22120.4\u22120. 20.00. 20.40.60.81.0T arget pose [m]T arget position over Time\nTarget x  position\nTarget y  position\nTarget z  position\n0 5 10 15 20 25 30\nT ime [s]\u22120.50.00.51.01.5Desired linear velocities 10\u22122\n [m/s]Desired linear velocities over Time\nDesired x  velocity\nDesired y  velocity\nDesired z  velocity\nFigure 6. Plot of an example of the desired Inverse Kinematic position\ncomputed as trajectory (left) and the desired linear velocities (right), with\ntrajectory time 10s.\n\u2022Obstacle Avoidance with the trajectory described by\nFig. 7. Similarly to [6], the sensors are treated like l\nfictitious springs with a rest length rland an associated\npseudo-energy. Once an obstacle is sensed, when the\nsensed distance dis smaller than the rest length of the\nspring, the task acts by pushing away the mobile base,\ntargeting zero pseudo-energy. If a sensor is activated, i.e.\nwhen d\u2264rk, the pseudo-energy associated with it is\n\u03f5l=1\n2(rl\u2212d)2, and \u03f5l= 0 otherwise. Like in the IK\ntask, a trajectory is followed in this case as well.\nBecause the robot uses LiDAR scanners instead of single-\nray range finders, the array of values is down-sampled to\nobtain a 180\u25e6map (even if the range of the scanner is\n270\u25e6), centered on the zero position (x, y)of the mobile\nbase through a map. The values sensed for each degree\nwere treated as a single unique range finder sensor. The\nangular values were constrained to the [\u221290\u25e6,90\u25e6]angle\nrange, as an overlap of two different sensors in the same\ndirection would have been interpreted by the robot as an\nobstacle sensed twice, leading to unstable behavior. In the\ncase of 0\u25e6and180\u25e6, where both sensors should be active,\nthe frontal sensor has priority over the rear sensor, which\nis turned off. This does not compromise the efficacy of the\ntask since the sensed obstacle is the same, but guarantees\nstability in the solution.\nWith this solution, the robot can sense obstacles allaround the base, downsampling the original 270\u25e6scanner\nrange in a 180\u25e6one. For this task, the desired velocity is\ngiven by:\n\u02d9qd(t) = J\u2020\no(q(t)) (\u02d9\u03c3d(t) +\u03b3CL(\u03c3d(t)\u2212\u03c3(q(t)))),\n(14)\nwhere \u03c3(q(t))\u2208Rlis the global pseudo-energy in the\ncurrent state and it is given by the sum of the pseudo-\nenergies \u03f5l(q(t))associated to all the msensors, namely\n\u03c3(q(t)) =Pm\nl=1\u03f5l(q(t)).Jo(q(t))is the Jacobian\nmatrix associated with this task and is computed with\npseudo-energy derivatives [6].\nUnlike the Jacobian J, which is always non-empty since\nthe arm has a mapping between Cartesian and operational\nvelocities at any time, the Jacobian Jo\u2208Rl\u00d7nfor this\ntask may be empty. If there are no obstacles closer than d\nto the robot, the matrix does not prevent subsequent tasks\nfrom being fully executed. As both sensors are active\nduring the execution of the task and are treated as range-\nfinder sensors for each degree of the half-circumference\nthat they describe, l= 181 + 179 = 360 in this specific\ncase. Thus, the sensors describe a full circumference\naround the robot.\nThe input z(t)of this task is given by the useful samples\nof the sensors, the target pseudo-energy, and its derivative.\nAs for the IK case, the target value and the desired\nvelocity are time-dependent since they are computed as\na trajectory of consecutive points.\nFigure 7. Obstacle Avoidance sequence of the mobile-YuMi research plat-\nform. The robot moves to a position behind the obstacle, finding a path that\nallows it to be far from it. Blue arrows show the behavior that the base would\nadopt in case of the IK task only, red ones show the overall control applied,\nincluding the obstacle avoidance task. Note that without the OA task, the\nrobot would have hit the obstacle.\n\u2022Maximization of the Manipulability measure . With this\ntask, the robot can maximize the manipulability of an arm\nby moving away from singularities.\nThe solution of the desired velocity is given by the\nfollowing equation:\n\u02d9qd(t) =\u03b3OL\u0012\u2202w(q(t))\n\u2202q(t)\u0013T\n, (15)\nwhere w(q(t))is the manipulability measure defined as\nw(q(t)) =q\ndet(J(q(t))JT(q(t))), (16)\nwhere J(q(t))is the Jacobian matrix of the kinematic\nchain involved in the task as a function of the actual\njoints configuration. The derivative of the manipulability\ncan be analytically computed as [10]:\n\u2202w(q)\n\u2202q=w\u00b7tr\u0012\n(JJT)\u22121\u2202J\n\u2202qJT\u0013\n, (17)\n\n--- Page 10 ---\nJOURNAL 10\nwhere the partial derivatives of the Jacobian were analyt-\nically calculated to speed up the computation.\nThus, the input z(t)of this task is simply given by the\nmanipulability measure, which is a scalar value.\n\u2022Maximization of distances from Mechanical Joint\nLimits (M.J.L.) as shown in Fig. 8. This task moves\neach joint as far as possible from its mechanical limits,\nexpressed as the minimum and maximum angles. As a\nconsequence, the joints tend to move toward their mean\nposition. It is important to note that it is guaranteed that if\na point of minimum is reached, it is the global one and not\na local one. The optimization process used in other tasks,\nfor example, in the maximization of the Manipulability,\nrelies on gradient-based methods. These methods adjust\nthe joint angles incrementally, following the direction\nthat most improves manipulability at each step. In that\nway, the manipulability maximization method would not\nexplore the entire configuration space, as in this lat-\nter case, considering nearby configurations only. As a\nresult, the algorithm stops if it reaches a point where\nno small change leads to further improvement. Those\npoints are local minima, namely the best solution within a\nsmall neighborhood, but not necessarily the best possible\nsolution overall (global minimum). Finding the global\nminimum would require a more exhaustive search of the\nfull configuration space, which is often computationally\ninfeasible for high-DOF systems, like redundant robotic\narms. Thus, the solution of those tasks and eventually the\ncost associated with them are strictly dependent on the\ninitial configuration of the arm.\nOn the other hand, in the maximization of distances from\nthe M.J.L. case, the maximum value achieved by the\ntask, i.e., the minimum cost, is not dependent on the\ninitial configuration, and it is a global point. This hap-\npens because, unlike manipulability, which depends on\ncomplex nonlinear interactions between joint angles and\nthe end-effector movement, the distance from mechanical\njoint limits is defined by a convex function. Each joint\u2019s\ndistance to its limits can be calculated independently,\nand the overall cost function has a well-defined, smooth\nstructure. This makes the optimization convex and thus\neasier to solve for global solutions. The desired velocities\nare computed as in the previous task (Eq. 15) where\nw(q(t)) =\u22121\n2nnX\ni=1\u0012qi(t)\u2212\u00afq\nqimax\u2212qimin\u00132\n(18)\nis the measure of the distance from mechanical joint\nlimits. qimax andqiminare, respectively, the maximum\nand minimum mechanical limits of the ithjoint for a\nkinematic chain of nelements.\nIn this case, as well, the analytical expression of the\nderivative was exploited to calculate the desired joint\nvelocities:\n\u2202w(q)\n\u2202q=\u22121\nnnX\ni=1\u0012qi\u2212\u00afq\nqimax\u2212qimin\u0013\n. (19)\nFigure 8. Maximization of the Distances from Mechanical Joint Limits\nsequence of the mobile-YuMi.\nThe input u(t)of this task is given by the measure w(t)\nand the constant values qmax andqmin.\nAll functions that solve the tasks return the desired velocity, a\nJacobian matrix used to apply the null space projection [26],\nand all useful parameters to compute the cost function or\nstudy the behavior of the execution. Except for the obstacle\navoidance task, all other tasks were implemented as described\nin [33].\nThe distracting tasks of the second group are designed to move\nthe arm or the base without achieving a meaningful objective.\nTwo of them are specific for the base, performing a circular\ntrajectory or rotating around the initial pose along the zaxis.\nThe other two control an arm, describing a circumference with\nthe end effector or moving the joints independently from each\nother, with a trajectory that oscillates from one mechanical\nbound to the other. The non-relevant tasks can return different\ntypes of Jacobians that can be selected. It is possible to have\na Jacobian matrix that prevents subsequent tasks from being\nexecuted completely or partially, or random Jacobians that lead\nto unpredictable behavior in the projection phase.\nB. Proposed costs\nThe cost set used in the experimental setup is composed of\nthe following:\n\u2022precision , which is given by the squared norm of the\npose error\ncost=||kd\u2212k||2. (20)\nThis can be divided into position precision and orientation\nprecision.\n\u2022safety , which can be computed with respect to all\ndistances lower than the rest length, applying pseudo-\nenergies as in the Obstacle Avoidance task, rk,\ncost=mX\nl=1(\n1\n2(d\u2212rl)2,ifdi< rl\n0, otherwise(21)\nor with respect to the minimum length\ncost=(\n1\n2(d\u2212rmin)2,ifd < r min\n0, otherwise. (22)\n\u2022maximization of the manipulability , which is given by\ncost=1\nw2\nmax.manip(23)\nwhere wmax.manip is the manipulability measure, which\nincreases as the arm moves further from singularities.\n\n--- Page 11 ---\nJOURNAL 11\n\u2022maximization of distance from mechanical joint limits ,\nwhich is given by\ncost=w2\nM.J.L. (24)\nwhere wM.J.L. is the measure of distance from mechan-\nical joint limits, which goes to zero as the joint angles\nreach their mean values.\n\u2022time , which is given by\ncost=t2(25)\nwhere tis the simulation time from the beginning of the\ntask to the end. Using t2increases the cost more sharply\nfor longer durations, which penalizes slow executions and\nencourages faster task completion \u2014 a common approach\nin optimization when efficiency is prioritized.\nIn the experiments, we normalize the costs to make them\ncomparable and combine them as in Eq. IV-C.\nC. Graphical user Interface G.U.I.\nTo facilitate ease-of-use and broaden accessibility, a dedi-\ncated Graphical User Interface (GUI) was developed as part\nof the system. The GUI is designed to provide an intuitive\nenvironment, allowing users to interact with the application\nwithout requiring deep technical expertise or command-line\ninteraction. By abstracting complex operations into simple\nvisual components such as buttons, drop-down menus, and\ninput fields, the GUI significantly streamlines the workflow.\nThe GUI reported in Fig. 9 allows users to configure key\nparameters for the simulation without requiring direct code\nmanipulation. It is divided into two main sections: cost param-\neters and simulation settings. In the cost parameters section,\nusers can assign relative weights to various evaluation criteria,\nincluding, for example, Accuracy and Safety. Each criterion\nhas an associated numeric input field where the user can\nFigure 9. Graphical User Interface for configuring learning simulation\nparameters. The interface allows users to define the relative weights of\ncost criteria (Accuracy, Safety, Manipulability, Speed) and set simulation\nparameters including population size, number of iterations, and simulation\nduration. Designed for ease of use, it supports intuitive navigation with\ndedicated controls for starting or exiting the simulation.\nFigure 10. Gazebo simulation of the model (left), real robot (right).\nspecify its weight, thereby customizing the learning objec-\ntives based on their priorities. Radio buttons indicate which\ncost criteria are active, guiding user attention and parameter\nselection.\nThe simulation settings section enables users to define the\nstructural aspects of the simulation. Users can set the number\nof stacks in the initial population, the number of iterations,\nand simulation time (in seconds) using clearly labeled input\nfields. The interface is minimalistic and designed with clarity\nand functionality in mind, facilitating the configuration of\nexperiments.\nD. Gazebo Simulation & Learning\nWe developed a framework in which tasks are simulated un-\nder controlled conditions, allowing us to analyze their behavior\nand correlations. Each task is associated with a set of learned\nparameters \u2014 such as estimated duration, success probability,\nresource consumption, and contextual constraints \u2014 which\nare used to compute a priority order based on a configurable\ncost function. Through repeated simulations, the framework\nlearns not only the characteristics of individual tasks but\nalso how they contribute to the overall mission efficiency.\nThis approach enables generalization across different real-\nworld environments. Since the priority order is derived from\ntask parameters and their role in fulfilling a given high-level\nmission \u2014 rather than from specific environmental features\n\u2014 it remains valid in any setting where the same mission\nand cost function apply. This decoupling of task logic from\nenvironmental details makes the framework robust, flexible,\nand applicable to a wide range of deployment scenarios. For\nthe simulation, we used a Gazebo model of the laboratory\nfacility at ABB Corporate Research. Both the environment\nand the robot model (Fig. 10 on the left) were provided by\nWARA Robotics1. The robot control functions and the Genetic\nProgramming algorithm were developed in Python.\nFor each different cost function, the learning phase is initial-\nized with a maximum number of generations \u2014 depending on\nthe learning complexity \u2014 and the number of individuals in\n1https://wara-robotics.se/\n\n--- Page 12 ---\nJOURNAL 12\nthe population. Then, each individual is a randomly generated\nSoT. The initial population can be created with random priority\norder, random parameters, or both, depending on the scope. All\nthe parameters are constrained to be in a range defined with\nprior knowledge on the tasks in order to maintain the stability\nof the model and have reasonable results.\nThen, the Genetic Programming algorithm evolves the individ-\nuals until a stability point is reached, namely an iteration in\nwhich all the survived SoT have small differences in cost. At\nthe end of each learning process, only the best SoT (i.e., the\none with the lowest cost in the final population) is considered\nto be the final result of the algorithm, and it is selected to\nbe tested. During the execution, if two competing SoTs have\nthe same associated cost, only one is randomly chosen to\nsurvive. In this way, we make sure to keep a constant number\nof individuals in the population for each generation.\nIn a first phase, the task parameters were fixed, and only the\norder of the tasks was allowed to be changed by the genetic\noperations. In this way, the priority order of the architecture\ncan be learned. Then, the priority order is fixed, and the\nparameters are allowed to change in a range constrained\nby boundaries dictated by previous knowledge on the tasks.\nFor example, the gain used in the definition of the desired\nvelocities \u02d9qdfor the Inverse Kinematic is restricted to assume\nvalues in the range \u03b3CL= (0,2][7, 3] because outside this\nrange there are no stability guarantees for that task.\nDuring the learning phase, the initial position of the arms can\nbe randomly initialized, with the constraint of keeping the end-\neffector in an area that avoids collisions at the beginning of\nthe simulation. Moreover, velocities applied to the arms are\nreduced towards position limits to not to overcome them and\nrisk damaging the robot:\n\u03f5(qimin\u2212qi)\u2264\u02d9qi\u2264\u03f5(qimax\u2212qi) (26)\nfor each ithjoint, with qiminits minimum allowed position\nandqimaxits maximum. Both static and dynamically changing\nenvironments were developed in order to allow the robot to\nreact to unpredictable changes in the environment. In the\nsecond case, a moving object was introduced in the Gazebo\nenvironment to simulate the presence of an operator close to\nthe robot to test the safety measures of the algorithm.\nFinally, non-relevant tasks are introduced in the dictionary\nof possible tasks to show the resilience of the algorithm to\nsuperfluous and distracting tasks with respect to its main goal.\nEven if it is not defined by the user, a high cost is associated\nwith a collision of the robot, and it is always included in case\nit happens.\nSince the LiDAR scanners are mounted away from the outer\nedge of the mobile base and positioned more centrally, a\nvariable offset is applied to the measured distances. This\ncorrection accounts for the physical displacement between\nthe scanner\u2019s position and the chassis, providing an accurate\nestimate of the distance from obstacles to the actual body of\nthe robot. If this distance is lower than a certain threshold, the\nrobot is considered to be damaged due to a collision, and the\ncost increases accordingly.E. Laboratory tests\nEach stack for the specific cost functions was tested in\na controlled laboratory environment, demonstrating that the\nproposed framework allows us to transfer the learned solution\nfrom the simulated environment to the real world. The robot\nis controlled through the ROS2 interface, feeding it with the\ndesired joint velocities for the arms, and the desired linear\n[x, y]and angular zvelocities for the mobile base.\nThe tests were conducted in the following settings:\n\u2022mobile base platform in a static environment,\n\u2022mobile base platform in a dynamically changing environ-\nment,\n\u2022one arm in a static environment,\n\u2022mobile base platform and one arm in a static environment,\n\u2022mobile base platform and one arm in a dynamically\nchanging environment.\nTo create a dynamically changing environment, a person was\nallowed to walk close to the robot while executing the task.\nDue to safety guarantees, the robot could always be stopped\nby the user while executing a task in a real-world scenario.\nVI. E XPERIMENTAL RESULTS\nIn this section, the results obtained during the learning\nand in subsequent laboratory tests are reported, evaluating the\nperformance and validity of the proposed methodology.\nA. Learning of the Stack of Tasks\nThe learning phase was divided into two parts: in the first\none, we learn the priority order of the task, and in the second,\nwe learn the parameters.\n1) Priority order Learning: In this phase, parameters were\nfixed with values that do not minimize the cost function given\nby the user, but they guarantee stability and the execution of\nall tasks.\nIn the case where only the mobile base is used, the problem\nis simpler as only two different tasks are relevant, as the\nmaximization of the manipulability and the distance from\nmechanical joint limits are manipulator-specific. During a\nsimulation, in case of collision with an obstacle, the cost is\nhigh, and so the robot prioritizes the Obstacle Avoidance task\nwith respect to the Inverse Kinematics, independently from\nthe global mission, in a few generations (2 or 3, depending on\nthe initial distribution). On the other hand, when the number\n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nGeneration60708090100% of stacks with final priority orderBase priority order\n1 2 3 4 5 6 7 8 9\nGeneration30405060708090100% of stacks with final priority orderBase priority order\nFigure 11. Example with the result of the % of individuals in a generation\nwith the same order as the final solution. The base case is reported on the\nleft, in which the order is [OA, IK]. On the right, the result obtained with the\nright arm + base, in which the order is [OA, IK, Max. Manip., Max MJL].\n\n--- Page 13 ---\nJOURNAL 13\nof tasks in the stack increases, the algorithm requires more\niterations to reach the final priority order (Fig. 11). Finally,\nthe final learned order is given by:\n\u20221stObstacle Avoidance\n\u20222ndInverse Kinematic\n\u20223rdMaximization of the Manipulability measure\n\u20224thMaximization of distance from M.J.L.\nAt times, the 3rdand4thpositions could be switched accord-\ning to the specific cost function. Nevertheless, this configura-\ntion helps to avoid singularities, making it a suitable option\nfor a general case. Furthermore, this order can be used as an\ninitial benchmark for the following phase, allowing the tasks\nto switch their positions and adapt to any cost function while\nlearning parameters. In this way, since the order is already\nalmost learned, the learning of the parameters would be faster.\n2) Parameters Learning: Once the initial priority order is\nfixed, it is possible to let the parameters assume any value in\nthe prior defined range. In this way, the algorithm is allowed to\nexploit as many combinations as possible, to find a minimum\nof the cost function. During this phase, the algorithm may take\ndifferent generations to reach a minimum point, depending on\nthe initial set of SoTs. On the basis of this, the solution derived\nat the end of the process may be different for the same task.\nAll simulations were stopped once a stationary behavior was\nreached. In this case, it is not possible to find a general solution\nthat may be adapted to all the cost functions, as was done\nfor the priority order. The parameters of a task are heavily\ndependent on the cost function defined by the user in terms\nof desired performance. To show how the adopted solution\nworks, we propose the following two examples.\nFirst, a cost function C1that focuses on some user require-\nments was decided:\nC1= 0.4\u00b7cprec.+ 0.5\u00b7cmin.dist. + 0.1\u00b7cmax.manip. .(27)\nThen, starting from the derived initial order, a random popula-\ntion was created. After that, the algorithm runs, and a station-\nary point is found after 7 generations. With this cost function,\nthe robot is encouraged to stay as far away from obstacles as\npossible while trying to reach the desired pose and maintaining\na high manipulability. At the end of the computation, the order\nof the SoT remains the same as the previously found one. All\nthe tasks involved in the fitness measure are still active, while\nthe last one (i.e., the maximization of distances from M.J.L.)\nis labeled as inactive, being not relevant for the execution of\nthe high-level mission. The purpose of the robot is to reach\na specific pose in the environment, avoiding dynamic and\nstatic obstacles. In this case, the learning was conducted in\na dynamic environment.\nIn Fig. 12, the best SoT \u2014 chosen by the optimization\nalgorithm \u2014 is reported. It is possible to observe how the\nObstacle Avoidance task is in first position, as in the case\nof collision, the price paid by the robot is higher than not\nreaching the desired pose.\nNote that the order and the parameters of the stack reported\nin Fig. 12 are consistent with the cost function described\nby the user. The higher priority task is Obstacle Avoidance,\nsince the robot wants to avoid collisions and stay far from1stOA (True)\nrk= 0.102, \u03b3CL= 1.726, ttraj.= 1.385\n2ndIK (True)\n\u03b3CL= 1.255, ttraj.= 3.609\n3rdMax. Manip. (True)\n\u03b3OL= 35.307\n4thMax. dist. M.J.L. (False)\n\u03b3OL= 15.001\nFigure 12. Example of a learned Stack of Tasks with cost function C1=\n0.4\u00b7cprec. + 0.5\u00b7cmin.dist. + 0.1\u00b7cmax.manip. .\nobstacles. Since the environment is dynamic, the time that is\nnecessary to avoid an obstacle is quite low, and the gain is high\nenough to guarantee a fast response. Then, Inverse Kinematics\nis prioritized. In this case, the gain is high enough to guarantee\nconvergence without instability, while the trajectory time is\nnot relevant. Maximization of the Manipulability task is after\nthe Inverse Kinematics one, allowing a maximization of the\nmeasure without compromising the desired pose. Also in\nthis case, the gain allows the robot to reach the objective\nwhile maintaining stability. Maximization of the distance from\nMechanical Joint Limits is in the last position, and even\ninactive, since it is not relevant in the minimization of the\ncost function.\nTo make a comparison, a second cost function can be defined:\nC2= 0.5\u00b7cprec.+ 0.5\u00b7ctime. (28)\nIn this case, the robot is not required to satisfy distances from\nobstacles but only to be precise and fast to achieve this result.\nThe resulting SoT from the learning phase, obtained after 6\ngenerations, is reported in Fig. 13. Also in this case, the order\nis the same as before. Despite not being strictly specified by\nthe user, the robot still avoids obstacles with maximum priority\nto avoid collisions. In fact, in the event of a collision, the robot\nis considered to be broken, and the assigned cost is high. In the\ncase of the Inverse Kinematic task, the gain is slightly higher,\nand the trajectory time is shorter, so the end-effector converges\nfaster to the desired pose. Note that the two maximization tasks\nare not relevant in the cost function, and then one is not active,\nwhile the other one has a low gain, so its involvement is quite\nweak in the solution of the high-level mission.\n3) Robustness to non-relevant tasks present in the dictio-\nnary tasks: In this phase, up to 4 non-relevant tasks present\nin the dictionary tasks were added to the possible SoT choices,\nin random positions, to show the resilience of the algorithm\nto the noise. These tasks prevent the robot from achieving\nits final goal if they have a priority order high enough to\nblock other useful tasks. Thus, the objective of the algorithm\nis to make them inactive or move them to the last positions\nin the SoT. We can state that the result is always reached,\nnamely that the non-relevant tasks are always rejected, but\nthe performance of the algorithm in doing this during the\n\n--- Page 14 ---\nJOURNAL 14\n1stOA (True)\nrk= 0.477, \u03b3CL= 1.257, ttraj.= 1.030\n2ndIK (True)\n\u03b3CL= 1.623, ttraj.= 2.641\n3rdMax. Manip. (True)\n\u03b3OL= 8.628\n4thMax. dist. M.J.L. (False)\n\u03b3OL= 51.074\nFigure 13. Example of a learned Stack of Tasks with cost function C2=\n0.5\u00b7cprec. + 0.5\u00b7ctime.\nlearning phase changes. The number of generations in which\nthis is achieved is heavily dependent on the initial random\nconfiguration or on the random nature of the crossover and\nmutation operations. If there is a high percentage of individuals\nin the population who feature non-relevant tasks in the first\npositions, the convergence will be slower. For example, a case\nfor a 10-DOF kinematic chain is reported in which the non-\nrelevant task starts at the first priority position. As can be seen\n1 2 3 4 5 6\nGeneration1.01.52.02.53.03.54.04.55.0Priority indexPriority index of the useles task\nFigure 14. Minimum priority order index for a non-relevant task.\nin Fig. 14, the task was rejected and moved to the last position.\nNote that 5this the last possible index. The curve obtained\ndoes not show a specific behavior, it is only a non-decreasing\nfunction, since all the improvements are subject to random\nchoices of the algorithm.\nB. Laboratory tests of the learned stacks of tasks\nAll the SoTs obtained during the learning phase were tested\non the robot in a laboratory environment, with the appropriate\nsafety measures. For this scope, the mobile base velocity was\nrestricted to the range [\u22120.2,0.2]m/s for linear velocities\nand[\u22120.2,0.2]rad/s for angular velocity. Moreover, joint\nvelocities were restricted to be in the range [\u22121,1]rad/s .\nThis choice clearly prevents the robot from satisfying a certain\ntrajectory in time if the specified time is too low. Because of\nthat, all trajectory times during the test phase were scaled with\na fixed offset. This choice does not affect the validity of our\nproposed framework.\nFor the experimental validation, a static environment with\nfixed obstacles was tested first. Then, a person was allowed\nto walk close to the robot (Fig. 16), testing the performancesin a dynamic environment and simulating a collaboration case\nbetween the operator and the robot.\nFigure 16. The mobile-YuMi research platform moving away from a human\noperator while navigating in the environment. On the left, it reaches a desired\npose in the laboratory, and then it moves away, avoiding collision with the\nuser as shown in the right picture, prioritizing safety at the cost of missing\nits target as learned in simulations.\nAs expected (as the Gazebo simulation of the mobile\nplatform and dual-arm YuMi robot is sufficiently accurate),\nthe learned SoTs also work in a real laboratory environment.\nAnyhow, due to some inaccuracies in the simulation, the\nresults are slightly different but still consistent. The resulting\nSoT was also tested under some different conditions, such as\na different initial configuration, different targets, or different\nobstacles. The result always satisfied the requirements, achiev-\ning the best performance in terms of the cost function.\nThe results of our lab experiments confirm that the task stack\nlearned entirely in simulation can be successfully applied in a\nreal-world environment. Despite variations in environmental\ndetails, the robot was able to execute the same high-level\nmission effectively, guided solely by the automatically derived\ntask priorities. This outcome validates the core premise of our\napproach: that a simulation-based learning process, driven by\ntask parameters and a cost function, produces control strategies\nthat are robust and generalizable. By decoupling task logic\nfrom environment-specific constraints, the framework enables\nadaptive behavior across deployment scenarios, significantly\nreducing engineering effort \u2014 being the SoT learned auto-\nmatically once the cost function is designed \u2014 and making\nrobotic systems more scalable and reusable.\nHowever, some limitations arise during tests with a real\nmachine. First, because of the noise and delay measurements\nfrom the sensors, the response to the introduction of a dy-\nnamic obstacle in the environment was not as fast as in the\nsimulations. Another issue that emerged during simulations\nis the lack of a global planner for the obstacle avoidance\ntask. Without it, the robot cannot navigate around obstacles\nintelligently and instead gets pushed as far away from them\nas possible \u2014 often moving unnecessarily far from its intended\ntarget. In Fig. 15, some results obtained during tests, with the\ntwo costs defined in previous sections, are reported.\nAn important point concerns the comparison with existing\napproaches. To the best of our knowledge, a direct quantitative\ncomparison with prior SoT methods is not feasible, as existing\nworks rely on manually defined, hard-coded task hierarchies\ntailored to specific scenarios. Nonetheless, the task priori-\nties autonomously learned by our method are qualitatively\naligned with those manually defined and adopted in classical\n\n--- Page 15 ---\nJOURNAL 15\n0 5 10 15 20 25 30 35 40\nT ime [s]1.52.02.53.03.5CostCost over T ime\nCost\n0 5 10 15 20 25 30 35 40\nT ime [s]\u22123\u22122\u221210123V elocities applied to the jointsV elocities applied to the joints over T ime\n\u0307\nq\n1 rad/s\n\u0307\nq\n2 rad/s\n\u0307\nq\n3 rad/s\n\u0307\nq\n4 rad/s\u0307\nq\n5 rad/s\n\u0307\nq\n6 rad/s\n\u0307\nq\n7 rad/s\n0 5 10 15 20 25 30 35 40\nT ime [s]\u22124\u22123\u22122\u22121012V elocities applied to the jointsV elocities applied to the joints over T ime\nbase \u0307\nq\n1 m/s\nbase \u0307\nq\n2 m/s\nbase \u0307\nq\n3 rad/s\n0 5 10 15 20 25 30 35 40\nT ime [s]\u22120.50.00.51.01.52.0P ose errorP ose error over T ime\ne\np\nx m\ne\np\ny m\ne\np\nz m\ne\nquat\nxe\nquat\ny\ne\nquat\nz\ne\nquat\nw\n0 10 20 30 40 50\nT ime [s]1.52.02.53.03.54.04.55.0CostCost over T ime\nCost\n0 10 20 30 40 50\nT ime [s]\u22122\u2212101234V elocities applied to the jointsV elocities applied to the joints over T ime\n\u0307\nq\n1 rad/s\n\u0307\nq\n2 rad/s\n\u0307\nq\n3 rad/s\n\u0307\nq\n4 rad/s\u0307\nq\n5 rad/s\n\u0307\nq\n6 rad/s\n\u0307\nq\n7 rad/s\n0 10 20 30 40 50\nT ime [s]\u22122\u221210123V elocities applied to the jointsV elocities applied to the joints over T ime\nbase \u0307\nq\n1 m/s\nbase \u0307\nq\n2 m/sbase \u0307\nq\n3 rad/s\n0 10 20 30 40 50\nT ime [s]\u22121.0\u22120.50.00.51.01.52.02.5P ose errorP ose error over T ime\ne\np\nx m\ne\np\ny m\ne\np\nz m\ne\nquat\nxe\nquat\ny\ne\nquat\nz\ne\nquat\nw\nFigure 15. In this picture, data obtained in laboratory tests with the mobile-YuMi research platform are reported for C1(above) and C2(below). From left\nto right, we reported the cost function C, velocities \u02d9qapplied to the arm joints and to the base, and the error of the pose for Inverse Kinematics. Note that,\nin the case of C1, since the minimum distance sensed by the LiDARs is included in the cost function, the fitness measure has some steps. These may occur\nwhen an object enters the range of the sensors or exits, updating the measure. Moreover, in this case, the cost reaches the minimum cost during execution. In\nthe case of C2, the distance with respect to objects in the environment is not included, but the robot does not collide in any case since the task is active. In\nthis case, the fitness measure always increases due to the square term in a function of time. For this eventuality, it is crucial to consider a task as completed\nand stop the time if the target is reached up to a threshold. In fact, after t= 10 seconds, it is possible to see the quadratic behavior of the time cost function,\nsince the precision cost is almost zero.\nframeworks for similar robotic missions, such as in [6].\nThis coherence with expert-designed strategies supports the\npractical validity of our learned configurations, despite their\nfully automated derivation.\nVII. C ONCLUSION AND FUTURE WORKS\nIn this work, we presented, to the best of our knowledge, the\nfirst approach that automatically learns a full Stack of Tasks\n(SoTs) \u2014 including task priorities, control parameters, and\nactivation logic \u2014 directly from a cost function defined by\nintuitive user preferences. Our method combines Genetic Pro-\ngramming and Reinforcement Learning to evolve interpretable\nand adaptive task execution hierarchies for redundant robotic\nsystems.\nWe demonstrated that Genetic Programming is an effective\nframework for optimizing both task order and control gains\nin response to a high-level objective function. This integration\nenables the robot to manage multiple tasks dynamically, pri-\noritizing safety-critical behaviors such as obstacle avoidance\nwhile maintaining overall precision and performance.\nExperimental validation \u2014 carried out in both simulation\n(Gazebo) and on a real dual-arm mobile robot (mobile-\nYuMi) \u2014 confirms that the learned SoTs generalize across\nvarying and unpredictable conditions without requiring manual\ntuning. Importantly, our framework supports zero-shot sim-\nto-real transfer: the SoTs learned entirely in simulation were\nsuccessfully deployed on the real system without fine-tuning.\nThis decoupling between environment-specific modeling and\ncontrol strategy marks a key advancement toward scalable,\nrobust robot autonomy. Despite these promising results, sev-\neral challenges remain. The simulation environment, while\ninstrumental for safe and rapid learning, is constrained by\nexecution speed and requires manual setup, limiting fullautomation. Bridging the simulation-to-reality gap remains\nan open issue, with sources of discrepancy including sensor\nnoise, communication delays, and the lack of a global ob-\nstacle avoidance planner. Moreover, while the cost function\neffectively guides task learning in simple scenarios, complex\ntask interactions and parameter dependencies merit deeper\nexploration. Future work will focus on: (i) designing richer\ncost models, (ii) integrating global planning layers [22] for\nmore robust navigation, (iii) extending the framework to multi-\nrobot coordination, and (iv) investigating adaptive, real-time\ntuning of control parameters based on sensory feedback.\nWe also plan to explore more sophisticated evolutionary\nstrategies and alternative genetic operators to accelerate con-\nvergence. Ultimately, we anticipate that advances in Genetic\nProgramming and RL will significantly enhance the generality,\nrobustness, and scalability of task execution architectures for\nredundant robotic systems operating in complex real-world\nenvironments.\nREFERENCES\n[1] Gianluca Antonelli, Filippo Arrichiello, and Stefano Chiaverini. \u201cThe\nnull-space-based behavioral control for autonomous robotic systems\u201d.\nIn:Intelligent Service Robotics 1.1 (2008), pp. 27\u201339.\n[2] Maumita Bhattacharya. \u201cEvolutionary Approaches to Expensive Op-\ntimisation\u201d. In: International Journal of Advanced Research in Arti-\nficial Intelligence 2 (Mar. 2013).\n[3] Magnus Bjerkeng et al. \u201cStability Analysis of a Hierarchical Archi-\ntecture for Discrete-Time Sensor-Based Control of Robotic Systems\u201d.\nIn:IEEE Transactions on Robotics 30.3 (2014), pp. 745\u2013753.\n[4] Paolo Di Lillo, Daniele Vito, and Gianluca Antonelli. \u201cMerging\nGlobal and Local Planners: Real-Time Replanning Algorithm of\nRedundant Robots Within a Task-Priority Framework\u201d. In: IEEE\nTransactions on Automation Science and Engineering PP (Jan. 2022),\npp. 1\u201314.\n[5] David Dominguez et al. \u201cA Stack-of-Tasks Approach Combined With\nBehavior Trees: A New Framework for Robot Control\u201d. In: IEEE\nRobotics and Automation Letters PP (Oct. 2022), pp. 1\u20138.\n\n--- Page 16 ---\nJOURNAL 16\n[6] Pietro Falco and Ciro Natale. \u201cLow-level flexible planning for mo-\nbile manipulators: A distributed perception approach\u201d. In: Advanced\nRobotics 28 (Oct. 2014).\n[7] Pietro Falco and Ciro Natale. \u201cOn the Stability of Closed-Loop\nInverse Kinematics Algorithms for Redundant Robots\u201d. In: Robotics,\nIEEE Transactions on 27 (Sept. 2011), pp. 780\u2013784.\n[8] Yongsheng Fang and Jun li. \u201cA Review of Tournament Selection in\nGenetic Programming\u201d. In: Oct. 2010, pp. 181\u2013192.\n[9] Mario Fiore et al. \u201cA General Framework for Hierarchical Redun-\ndancy Resolution Under Arbitrary Constraints\u201d. In: IEEE Transac-\ntions on Robotics PP (June 2023), pp. 1\u201320.\n[10] Jesse Haviland and Peter Corke. Maximising Manipulability During\nResolved-Rate Motion Control . Feb. 2020.\n[11] Torsten Hildebrandt and J \u00a8urgen Branke. \u201cOn Using Surrogates with\nGenetic Programming\u201d. In: Evolutionary Computation 23.3 (Sept.\n2015), pp. 343\u2013367.\n[12] John I and Riccardo Poli. \u201cA Genetic Programming Tutorial\u201d. In:\n(June 2003).\n[13] Matteo Iovino et al. \u201cA Framework for Learning Behavior Trees in\nCollaborative Robotic Applications\u201d. In: 2023 IEEE 19th Interna-\ntional Conference on Automation Science and Engineering (CASE) .\n2023, pp. 1\u20138.\n[14] Matteo Iovino et al. \u201cLearning Behavior Trees with Genetic Program-\nming in Unpredictable Environments\u201d. In: 2021 IEEE International\nConference on Robotics and Automation (ICRA) . 2021, pp. 4591\u2013\n4597.\n[15] Sourabh Katoch, Sumit Chauhan, and Vijay Chahar. \u201cA review on\ngenetic algorithm: past, present, and future\u201d. In: Multimedia Tools\nand Applications 80 (Oct. 2020), pp. 8091\u20138126.\n[16] Sunny Katyara et al. Formulating Intuitive Stack-of-Tasks with Visuo-\nTactile Perception for Collaborative Human-Robot Fine Manipula-\ntion. Mar. 2021.\n[17] Asifullah Khan et al. \u201cA recent survey on the applications of genetic\nprogramming in image processing\u201d. In: Computational Intelligence\n37 (June 2021).\n[18] Mohammad Wahab Khan. \u201cA Comprehensive Survey of Genetic\nProgramming Applications in Modern Biological Research\u201d. In: Med-\ninformatics (Dec. 2024).\n[19] Jens Kober, J. Bagnell, and Jan Peters. \u201cReinforcement Learning\nin Robotics: A Survey\u201d. In: The International Journal of Robotics\nResearch 32 (Sept. 2013), pp. 1238\u20131274.\n[20] Azizjon Kobilov and Jianglin Lan. Automatic Robot Task Planning by\nIntegrating Large Language Model with Genetic Programming . Feb.\n2025.\n[21] Michael Kommenda et al. \u201cEvolving Simple Symbolic Regression\nModels by Multi-Objective Genetic Programming\u201d. In: Genetic Pro-\ngramming Theory and Practice XIII . Ed. by Rick Riolo et al. Cham:\nSpringer International Publishing, 2016, pp. 1\u201319.\n[22] Steve Macenski et al. The Marathon 2: A Navigation System . Feb.\n2020.\n[23] Nicolas Mansard, Oussama Khatib, and Abderrahmane Kheddar. \u201cA\nUnified Approach to Integrate Unilateral Constraints in the Stack of\nTasks\u201d. In: IEEE Transactions on Robotics 25.3 (2009), pp. 670\u2013685.\n[24] Nicolas Mansard et al. \u201cA versatile Generalized Inverted Kinematics\nimplementation for collaborative working humanoid robots: The Stack\nOf Tasks\u201d. In: July 2009, pp. 1\u20136.\n[25] Francesco Marchetti and Edmondo Minisci. \u201cA Hybrid Neural\nNetwork-Genetic Programming Intelligent Control Approach\u201d. In:\nNov. 2020.\n[26] Signe Moe et al. \u201cSet-Based Tasks within the Singularity-Robust\nMultiple Task-Priority Inverse Kinematics Framework: General For-\nmulation, Stability Analysis, and Experimental Results\u201d. In: Frontiers\nin Robotics and AI 3 (Apr. 2016).\n[27] A.C. Nearchou and N.A. Aspragathos. \u201cApplication of genetic al-\ngorithms to point-to-point motion of redundant manipulators\u201d. In:\nMechanism and Machine Theory 31.3 (1996), pp. 261\u2013270.\n[28] Andreas Nearchou. \u201cSolving the inverse kinematics problem of re-\ndundant robots operating in complex environments via a modified\ngenetic algorithm\u201d. In: Mechanism and Machine Theory 33 (Apr.\n1998), pp. 273\u2013292.\n[29] Peter Nordin and Frank Francone. \u201cExplicitly Defined Introns and\nDestructive Crossover in Genetic Programming\u201d. In: (July 1995).\n[30] Arman Oliazadeh et al. \u201cGenetic Programming (GP): An Introduction\nand Practical Application\u201d. In: July 2022, pp. 251\u2013271.\n[31] Oscar E. Ramos et al. \u201cDynamic motion capture and edition using a\nstack of tasks\u201d. In: 2011 11th IEEE-RAS International Conference on\nHumanoid Robots . 2011, pp. 224\u2013230.[32] Ashish Kumar Shakya, Gopinatha Pillai, and Sohom Chakrabarty.\n\u201cReinforcement learning algorithms: A brief survey\u201d. In: Expert\nSystems with Applications 231 (2023), p. 120495.\n[33] Bruno Siciliano et al. Robotics: Modelling, Planning and Control . 1st.\nSpringer Publishing Company, Incorporated, 2008.\n[34] M.C. Sinclair and S.H. Shami. \u201cEvolving simple software agents:\ncomparing genetic algorithm and genetic programming performance\u201d.\nIn:Second International Conference On Genetic Algorithms In Engi-\nneering Systems: Innovations And Applications . 1997, pp. 421\u2013426.\n[35] L \u00b4eo Sotto et al. \u201cA Study on Graph Representations for Genetic\nProgramming\u201d. In: June 2020.\n[36] Sotiris Stavridis, Pietro Falco, and Zoe Doulgeri. \u201cPick-and-place in\ndynamic environments with a mobile dual-arm robot equipped with\ndistributed distance sensors\u201d. In: July 2021.\n[37] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An\nIntroduction . Second. The MIT Press, 2018.\n[38] Niccol `o Turcato et al. Towards Autonomous Reinforcement Learning\nfor Real-World Robotic Manipulation with Large Language Models .\nMar. 2025.\n[39] Itai Tzruia et al. Fitness Approximation through Machine Learning .\nSept. 2023.\n[40] Mark Wineberg and Franz Oppacher. \u201cThe benefits of computing with\nintrons\u201d. In: Proceedings of the 1st Annual Conference on Genetic\nProgramming . Stanford, California: MIT Press, 1996, pp. 410\u2013415.\n[41] Fangfang Zhang et al. \u201cSurvey on Genetic Programming and Machine\nLearning Techniques for Heuristic Design in Job Shop Scheduling\u201d.\nIn:IEEE Transactions on Evolutionary Computation PP (Jan. 2023),\npp. 1\u20131.\n[42] Xiangbing Zhou et al. \u201cMulti-strategy competitive-cooperative co-\nevolutionary algorithm and its application\u201d. In: Information Sciences\n635 (Mar. 2023).",
  "project_dir": "artifacts/projects/enhanced_cs.SY_2508.10780v1_Learning_Task_Execution_Hierarchies_for_Redundant_",
  "communication_dir": "artifacts/projects/enhanced_cs.SY_2508.10780v1_Learning_Task_Execution_Hierarchies_for_Redundant_/.agent_comm",
  "assigned_at": "2025-08-16T20:58:55.515817",
  "status": "assigned"
}